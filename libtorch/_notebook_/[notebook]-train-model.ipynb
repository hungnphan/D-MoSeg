{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe06d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// If you want to add include path\n",
    "#pragma cling add_include_path(\"/home/hung/libtorch/include\")\n",
    "#pragma cling add_include_path(\"/home/hung/libtorch/include/torch/csrc/api/include\")\n",
    "\n",
    "// If you want to add library path\n",
    "#pragma cling add_library_path(\"/home/hung/libtorch/lib\")\n",
    "\n",
    "// If you want to load library for CPU (default)\n",
    "#pragma cling load(\"libc10.so\")\n",
    "#pragma cling load(\"libcaffe2_detectron_ops_gpu.so\")\n",
    "#pragma cling load(\"libcaffe2_module_test_dynamic.so\")\n",
    "#pragma cling load(\"libcaffe2_observers.so\")\n",
    "#pragma cling load(\"libfmt.so\")\n",
    "#pragma cling load(\"libfmt.so.7\")\n",
    "#pragma cling load(\"libfmt.so.7.0.3\")\n",
    "#pragma cling load(\"libgomp-75eea7e8.so.1\")\n",
    "#pragma cling load(\"libjitbackend_test.so\")\n",
    "#pragma cling load(\"libprocess_group_agent.so\")\n",
    "#pragma cling load(\"libshm.so\")\n",
    "#pragma cling load(\"libtensorpipe_agent.so\")\n",
    "#pragma cling load(\"libtorch.so\")\n",
    "#pragma cling load(\"libtorchbind_test.so\")\n",
    "#pragma cling load(\"libtorch_cpu.so\")\n",
    "#pragma cling load(\"libtorch_global_deps.so\")\n",
    "\n",
    "// If you use GPU, add more libraries\n",
    "#pragma cling load(\"libc10_cuda.so\")\n",
    "#pragma cling load(\"libcaffe2_nvrtc.so\")\n",
    "#pragma cling load(\"libcudart-1b201d85.so.10.1\")\n",
    "#pragma cling load(\"libnvrtc-5e8a26c9.so.10.1\")\n",
    "#pragma cling load(\"libnvrtc-builtins.so\")\n",
    "#pragma cling load(\"libnvToolsExt-3965bdd0.so.1\")\n",
    "#pragma cling load(\"libtorch_cuda.so\")\n",
    "#pragma cling load(\"libc10d_cuda_test.so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4bec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "// We add include path and library path for OpenCV\n",
    "#pragma cling add_include_path(\"/usr/local/include/opencv4\")\n",
    "#pragma cling add_include_path(\"/usr/local/include/opencv4/opencv2\")\n",
    "#pragma cling add_library_path(\"/usr/local/lib\")\n",
    "\n",
    "// We load lib for OpenCV\n",
    "#pragma cling load(\"libopencv_alphamat.so\")\n",
    "#pragma cling load(\"libopencv_alphamat.so.4.5\")\n",
    "#pragma cling load(\"libopencv_alphamat.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_aruco.so\")\n",
    "#pragma cling load(\"libopencv_aruco.so.4.5\")\n",
    "#pragma cling load(\"libopencv_aruco.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_bgsegm.so\")\n",
    "#pragma cling load(\"libopencv_bgsegm.so.4.5\")\n",
    "#pragma cling load(\"libopencv_bgsegm.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_bioinspired.so\")\n",
    "#pragma cling load(\"libopencv_bioinspired.so.4.5\")\n",
    "#pragma cling load(\"libopencv_bioinspired.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_calib3d.so\")\n",
    "#pragma cling load(\"libopencv_calib3d.so.4.5\")\n",
    "#pragma cling load(\"libopencv_calib3d.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_ccalib.so\")\n",
    "#pragma cling load(\"libopencv_ccalib.so.4.5\")\n",
    "#pragma cling load(\"libopencv_ccalib.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_core.so\")\n",
    "#pragma cling load(\"libopencv_core.so.4.5\")\n",
    "#pragma cling load(\"libopencv_core.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_cudaarithm.so\")\n",
    "#pragma cling load(\"libopencv_cudaarithm.so.4.5\")\n",
    "#pragma cling load(\"libopencv_cudaarithm.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_cudabgsegm.so\")\n",
    "#pragma cling load(\"libopencv_cudabgsegm.so.4.5\")\n",
    "#pragma cling load(\"libopencv_cudabgsegm.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_cudafeatures2d.so\")\n",
    "#pragma cling load(\"libopencv_cudafeatures2d.so.4.5\")\n",
    "#pragma cling load(\"libopencv_cudafeatures2d.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_cudafilters.so\")\n",
    "#pragma cling load(\"libopencv_cudafilters.so.4.5\")\n",
    "#pragma cling load(\"libopencv_cudafilters.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_cudaimgproc.so\")\n",
    "#pragma cling load(\"libopencv_cudaimgproc.so.4.5\")\n",
    "#pragma cling load(\"libopencv_cudaimgproc.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_cudalegacy.so\")\n",
    "#pragma cling load(\"libopencv_cudalegacy.so.4.5\")\n",
    "#pragma cling load(\"libopencv_cudalegacy.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_cudaobjdetect.so\")\n",
    "#pragma cling load(\"libopencv_cudaobjdetect.so.4.5\")\n",
    "#pragma cling load(\"libopencv_cudaobjdetect.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_cudaoptflow.so\")\n",
    "#pragma cling load(\"libopencv_cudaoptflow.so.4.5\")\n",
    "#pragma cling load(\"libopencv_cudaoptflow.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_cudastereo.so\")\n",
    "#pragma cling load(\"libopencv_cudastereo.so.4.5\")\n",
    "#pragma cling load(\"libopencv_cudastereo.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_cudawarping.so\")\n",
    "#pragma cling load(\"libopencv_cudawarping.so.4.5\")\n",
    "#pragma cling load(\"libopencv_cudawarping.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_cudev.so\")\n",
    "#pragma cling load(\"libopencv_cudev.so.4.5\")\n",
    "#pragma cling load(\"libopencv_cudev.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_datasets.so\")\n",
    "#pragma cling load(\"libopencv_datasets.so.4.5\")\n",
    "#pragma cling load(\"libopencv_datasets.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_dnn.so\")\n",
    "#pragma cling load(\"libopencv_dnn.so.4.5\")\n",
    "#pragma cling load(\"libopencv_dnn.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_dnn_objdetect.so\")\n",
    "#pragma cling load(\"libopencv_dnn_objdetect.so.4.5\")\n",
    "#pragma cling load(\"libopencv_dnn_objdetect.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_dnn_superres.so\")\n",
    "#pragma cling load(\"libopencv_dnn_superres.so.4.5\")\n",
    "#pragma cling load(\"libopencv_dnn_superres.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_dpm.so\")\n",
    "#pragma cling load(\"libopencv_dpm.so.4.5\")\n",
    "#pragma cling load(\"libopencv_dpm.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_face.so\")\n",
    "#pragma cling load(\"libopencv_face.so.4.5\")\n",
    "#pragma cling load(\"libopencv_face.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_features2d.so\")\n",
    "#pragma cling load(\"libopencv_features2d.so.4.5\")\n",
    "#pragma cling load(\"libopencv_features2d.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_flann.so\")\n",
    "#pragma cling load(\"libopencv_flann.so.4.5\")\n",
    "#pragma cling load(\"libopencv_flann.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_freetype.so\")\n",
    "#pragma cling load(\"libopencv_freetype.so.4.5\")\n",
    "#pragma cling load(\"libopencv_freetype.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_fuzzy.so\")\n",
    "#pragma cling load(\"libopencv_fuzzy.so.4.5\")\n",
    "#pragma cling load(\"libopencv_fuzzy.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_gapi.so\")\n",
    "#pragma cling load(\"libopencv_gapi.so.4.5\")\n",
    "#pragma cling load(\"libopencv_gapi.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_hdf.so\")\n",
    "#pragma cling load(\"libopencv_hdf.so.4.5\")\n",
    "#pragma cling load(\"libopencv_hdf.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_hfs.so\")\n",
    "#pragma cling load(\"libopencv_hfs.so.4.5\")\n",
    "#pragma cling load(\"libopencv_hfs.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_highgui.so\")\n",
    "#pragma cling load(\"libopencv_highgui.so.4.5\")\n",
    "#pragma cling load(\"libopencv_highgui.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_img_hash.so\")\n",
    "#pragma cling load(\"libopencv_img_hash.so.4.5\")\n",
    "#pragma cling load(\"libopencv_img_hash.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_imgcodecs.so\")\n",
    "#pragma cling load(\"libopencv_imgcodecs.so.4.5\")\n",
    "#pragma cling load(\"libopencv_imgcodecs.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_imgproc.so\")\n",
    "#pragma cling load(\"libopencv_imgproc.so.4.5\")\n",
    "#pragma cling load(\"libopencv_imgproc.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_intensity_transform.so\")\n",
    "#pragma cling load(\"libopencv_intensity_transform.so.4.5\")\n",
    "#pragma cling load(\"libopencv_intensity_transform.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_line_descriptor.so\")\n",
    "#pragma cling load(\"libopencv_line_descriptor.so.4.5\")\n",
    "#pragma cling load(\"libopencv_line_descriptor.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_mcc.so\")\n",
    "#pragma cling load(\"libopencv_mcc.so.4.5\")\n",
    "#pragma cling load(\"libopencv_mcc.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_ml.so\")\n",
    "#pragma cling load(\"libopencv_ml.so.4.5\")\n",
    "#pragma cling load(\"libopencv_ml.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_objdetect.so\")\n",
    "#pragma cling load(\"libopencv_objdetect.so.4.5\")\n",
    "#pragma cling load(\"libopencv_objdetect.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_optflow.so\")\n",
    "#pragma cling load(\"libopencv_optflow.so.4.5\")\n",
    "#pragma cling load(\"libopencv_optflow.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_phase_unwrapping.so\")\n",
    "#pragma cling load(\"libopencv_phase_unwrapping.so.4.5\")\n",
    "#pragma cling load(\"libopencv_phase_unwrapping.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_photo.so\")\n",
    "#pragma cling load(\"libopencv_photo.so.4.5\")\n",
    "#pragma cling load(\"libopencv_photo.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_plot.so\")\n",
    "#pragma cling load(\"libopencv_plot.so.4.5\")\n",
    "#pragma cling load(\"libopencv_plot.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_quality.so\")\n",
    "#pragma cling load(\"libopencv_quality.so.4.5\")\n",
    "#pragma cling load(\"libopencv_quality.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_rapid.so\")\n",
    "#pragma cling load(\"libopencv_rapid.so.4.5\")\n",
    "#pragma cling load(\"libopencv_rapid.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_reg.so\")\n",
    "#pragma cling load(\"libopencv_reg.so.4.5\")\n",
    "#pragma cling load(\"libopencv_reg.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_rgbd.so\")\n",
    "#pragma cling load(\"libopencv_rgbd.so.4.5\")\n",
    "#pragma cling load(\"libopencv_rgbd.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_saliency.so\")\n",
    "#pragma cling load(\"libopencv_saliency.so.4.5\")\n",
    "#pragma cling load(\"libopencv_saliency.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_sfm.so\")\n",
    "#pragma cling load(\"libopencv_sfm.so.4.5\")\n",
    "#pragma cling load(\"libopencv_sfm.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_shape.so\")\n",
    "#pragma cling load(\"libopencv_shape.so.4.5\")\n",
    "#pragma cling load(\"libopencv_shape.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_stereo.so\")\n",
    "#pragma cling load(\"libopencv_stereo.so.4.5\")\n",
    "#pragma cling load(\"libopencv_stereo.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_stitching.so\")\n",
    "#pragma cling load(\"libopencv_stitching.so.4.5\")\n",
    "#pragma cling load(\"libopencv_stitching.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_structured_light.so\")\n",
    "#pragma cling load(\"libopencv_structured_light.so.4.5\")\n",
    "#pragma cling load(\"libopencv_structured_light.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_superres.so\")\n",
    "#pragma cling load(\"libopencv_superres.so.4.5\")\n",
    "#pragma cling load(\"libopencv_superres.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_surface_matching.so\")\n",
    "#pragma cling load(\"libopencv_surface_matching.so.4.5\")\n",
    "#pragma cling load(\"libopencv_surface_matching.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_text.so\")\n",
    "#pragma cling load(\"libopencv_text.so.4.5\")\n",
    "#pragma cling load(\"libopencv_text.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_tracking.so\")\n",
    "#pragma cling load(\"libopencv_tracking.so.4.5\")\n",
    "#pragma cling load(\"libopencv_tracking.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_video.so\")\n",
    "#pragma cling load(\"libopencv_video.so.4.5\")\n",
    "#pragma cling load(\"libopencv_video.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_videoio.so\")\n",
    "#pragma cling load(\"libopencv_videoio.so.4.5\")\n",
    "#pragma cling load(\"libopencv_videoio.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_videostab.so\")\n",
    "#pragma cling load(\"libopencv_videostab.so.4.5\")\n",
    "#pragma cling load(\"libopencv_videostab.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_wechat_qrcode.so\")\n",
    "#pragma cling load(\"libopencv_wechat_qrcode.so.4.5\")\n",
    "#pragma cling load(\"libopencv_wechat_qrcode.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_xfeatures2d.so\")\n",
    "#pragma cling load(\"libopencv_xfeatures2d.so.4.5\")\n",
    "#pragma cling load(\"libopencv_xfeatures2d.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_ximgproc.so\")\n",
    "#pragma cling load(\"libopencv_ximgproc.so.4.5\")\n",
    "#pragma cling load(\"libopencv_ximgproc.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_xobjdetect.so\")\n",
    "#pragma cling load(\"libopencv_xobjdetect.so.4.5\")\n",
    "#pragma cling load(\"libopencv_xobjdetect.so.4.5.2\")\n",
    "#pragma cling load(\"libopencv_xphoto.so\")\n",
    "#pragma cling load(\"libopencv_xphoto.so.4.5\")\n",
    "#pragma cling load(\"libopencv_xphoto.so.4.5.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c694d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <torch/torch.h>\n",
    "#include <opencv2/opencv.hpp>\n",
    "\n",
    "#include <iostream>\n",
    "#include <cstdio>\n",
    "#include <cmath>\n",
    "#include <cstring>\n",
    "#include <string>\n",
    "#include <vector>\n",
    "#include <map>\n",
    "#include <utility>\n",
    "#include <sstream>\n",
    "#include <iomanip>\n",
    "#include <tuple>\n",
    "#include <stdint.h>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f721f9c",
   "metadata": {},
   "source": [
    "# Some util functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939e806",
   "metadata": {},
   "source": [
    "## (a) torch::Tensor --> cv::Mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2589519",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv::Mat cvt_torchTensor_to_cvMat(torch::Tensor& tsr){\n",
    "    cv::Mat mat;\n",
    "    // BGR image\n",
    "    if(tsr.dim() == 3)        \n",
    "        mat = cv::Mat(tsr.size(1), tsr.size(0), CV_8UC3);\n",
    "    // Grayscale image\n",
    "    else if(tsr.dim() == 2)\n",
    "        mat = cv::Mat(tsr.size(1), tsr.size(0), CV_8U);\n",
    "        \n",
    "    torch::Tensor flatten_tsr = tsr.contiguous();\n",
    "    std::memcpy(mat.data, (uint8_t*) flatten_tsr.data_ptr(), sizeof(unsigned char) * tsr.numel());\n",
    "    return mat;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aad747",
   "metadata": {},
   "source": [
    "## (b) cv::Mat --> torch:Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "390c30df",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch::Tensor cvt_cvMat_to_torchTensor(cv::Mat& mat){\n",
    "    torch::Tensor tsr;\n",
    "    // BGR image\n",
    "    if(mat.channels() == 3){    \n",
    "        tsr = torch::zeros({mat.cols, mat.rows, mat.channels()},\n",
    "                                         torch::TensorOptions(torch::kUInt8) );\n",
    "        memcpy(tsr.data_ptr(), mat.data, tsr.numel() * sizeof(unsigned char));    \n",
    "    }\n",
    "    // Grayscale image\n",
    "    else if(mat.channels() == 1){   \n",
    "        tsr = torch::zeros({mat.cols, mat.rows},\n",
    "                                         torch::TensorOptions(torch::kUInt8) );\n",
    "        memcpy(tsr.data_ptr(), mat.data, tsr.numel() * sizeof(unsigned char));\n",
    "    }\n",
    "    return tsr;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da138c",
   "metadata": {},
   "source": [
    "# Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7d7cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset : public torch::data::datasets::Dataset<CustomDataset> {\n",
    "\n",
    "public:\n",
    "    CustomDataset(int            proc_id, \n",
    "                  int            num_proc,\n",
    "                  int            n_replicas,\n",
    "                  std::string    data_dir, \n",
    "                  std::string    scenario_name, \n",
    "                  std::string    sequence_name)\n",
    "      : proc_id          {proc_id},    \n",
    "        num_proc         {num_proc},\n",
    "        n_replicas       {n_replicas},\n",
    "        data_dir         {data_dir},\n",
    "        scenario_name    {scenario_name},\n",
    "        sequence_name    {sequence_name},\n",
    "        sample_indices   {std::vector<int>()}{\n",
    "            \n",
    "        // split number of frames for each worker\n",
    "        this->split_data_for_workers();\n",
    "    }\n",
    "\n",
    "    torch::data::Example<> get(size_t index) override{\n",
    "        int data_idx = sample_indices[index];\n",
    "        // std::cout << \"Getting data of index #\" << data_idx << std::endl;\n",
    "        \n",
    "        // Convert int to string with leading zeros\n",
    "        std::stringstream ss;\n",
    "        ss << std::setw(6) << std::setfill('0') << data_idx;\n",
    "        std::string index_str = ss.str();\n",
    "        \n",
    "        // Path to frame data\n",
    "        std::string path_to_data = this->data_dir + \"/\" \n",
    "            + this->scenario_name + \"/\" \n",
    "            + this->sequence_name + \"/\"\n",
    "            + index_str;\n",
    "                \n",
    "        // Read in, bg, fg images from opencv\n",
    "        cv::Mat in_img = cv::imread(path_to_data + \"_in.png\", cv::IMREAD_COLOR);\n",
    "        cv::Mat bg_img = cv::imread(path_to_data + \"_bg.png\", cv::IMREAD_COLOR);\n",
    "        cv::Mat fg_img = cv::imread(path_to_data + \"_fg.png\", cv::IMREAD_GRAYSCALE);\n",
    "                \n",
    "        // Convert cv::Mat to torch::Tensor\n",
    "        torch::Tensor in_tsr = cvt_cvMat_to_torchTensor(in_img); // [W, H, 3]\n",
    "        torch::Tensor bg_tsr = cvt_cvMat_to_torchTensor(bg_img); // [W, H, 3]\n",
    "        torch::Tensor fg_tsr = cvt_cvMat_to_torchTensor(fg_img).unsqueeze(-1); // [W, H, 1]\n",
    "        \n",
    "        torch::Tensor data = torch::cat({in_tsr, bg_tsr}, -1);  // [W, H, 6]\n",
    "         \n",
    "        // Transpose: [N,W,H,C] -> [N,C,W,H]\n",
    "        data = data.permute({2,0,1}).contiguous();\n",
    "        fg_tsr = fg_tsr.permute({2,0,1}).contiguous();       \n",
    "        \n",
    "        return {data, fg_tsr};  \n",
    "    }\n",
    "\n",
    "    torch::optional<size_t> size() const override {\n",
    "        return sample_indices.size();\n",
    "    }\n",
    "    \n",
    "private:\n",
    "    void split_data_for_workers(){\n",
    "        int nFrames = 200;\n",
    "        \n",
    "        for(int rep=0;rep<this->n_replicas;rep++){\n",
    "            // Iterate through a data sequence:\n",
    "            // Frame index: 1,2,3,4,...200\n",
    "            for(int frame_idx=1;frame_idx<=nFrames;frame_idx++){\n",
    "                if((frame_idx-1) % num_proc == proc_id)\n",
    "                    sample_indices.push_back(frame_idx);\n",
    "            }\n",
    "        }\n",
    "        return;\n",
    "    }    \n",
    "    \n",
    "    int                 proc_id;\n",
    "    int                 num_proc;\n",
    "    int                 n_replicas;\n",
    "    \n",
    "    std::string         data_dir;\n",
    "    std::string         scenario_name;\n",
    "    std::string         sequence_name;\n",
    "    \n",
    "    std::vector<int>    sample_indices;\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03622a",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65188601",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Define a Convolutional Module\n",
    "struct Model : torch::nn::Module {\n",
    "  Model()\n",
    "    : dconv1       (torch::nn::Conv2dOptions(6,6,3).stride(1).padding(1).groups(6).bias(false)),\n",
    "      pconv1       (torch::nn::Conv2dOptions(6,16,1).stride(1).groups(1).bias(false)),\n",
    "      dconv2       (torch::nn::Conv2dOptions(16,16,3).stride(1).padding(1).groups(16).bias(false)),\n",
    "      pconv2       (torch::nn::Conv2dOptions(16,16,1).stride(1).groups(1).bias(false)),\n",
    "      dconv3       (torch::nn::Conv2dOptions(16,16,3).stride(1).padding(1).groups(16).bias(false)),\n",
    "      pconv3       (torch::nn::Conv2dOptions(16,16,1).stride(1).groups(1).bias(false)),    \n",
    "      dconv4       (torch::nn::Conv2dOptions(16,16,3).stride(1).padding(1).groups(16).bias(false)),\n",
    "      pconv4       (torch::nn::Conv2dOptions(16,16,1).stride(1).groups(1).bias(false)),    \n",
    "      dconv5       (torch::nn::Conv2dOptions(16,16,3).stride(1).padding(1).groups(16).bias(false)),\n",
    "      pconv5       (torch::nn::Conv2dOptions(16,16,1).stride(1).groups(1).bias(false)), \n",
    "      dconv6       (torch::nn::Conv2dOptions(16,16,3).stride(1).padding(1).groups(16).bias(false)),\n",
    "      pconv6       (torch::nn::Conv2dOptions(16,16,1).stride(1).groups(1).bias(false)),    \n",
    "      dconv7       (torch::nn::Conv2dOptions(16,16,3).stride(1).padding(1).groups(16).bias(false)),\n",
    "      pconv7       (torch::nn::Conv2dOptions(16,16,1).stride(1).groups(1).bias(false)),    \n",
    "      dconv8       (torch::nn::Conv2dOptions(16,16,3).stride(1).padding(1).groups(16).bias(false)),\n",
    "      pconv8       (torch::nn::Conv2dOptions(16,1,1).stride(1).groups(1).bias(false)) {\n",
    "    \n",
    "    register_module(\"dconv1\", dconv1);\n",
    "    register_module(\"pconv1\", pconv1);\n",
    "    register_module(\"dconv2\", dconv2);\n",
    "    register_module(\"pconv2\", pconv2);\n",
    "    register_module(\"dconv3\", dconv3);\n",
    "    register_module(\"pconv3\", pconv3);\n",
    "    register_module(\"dconv4\", dconv4);\n",
    "    register_module(\"pconv4\", pconv4);\n",
    "    register_module(\"dconv5\", dconv5);\n",
    "    register_module(\"pconv5\", pconv5);\n",
    "    register_module(\"dconv6\", dconv6);\n",
    "    register_module(\"pconv6\", pconv6);\n",
    "    register_module(\"dconv7\", dconv7);\n",
    "    register_module(\"pconv7\", pconv7);\n",
    "    register_module(\"dconv8\", dconv8);\n",
    "    register_module(\"pconv8\", pconv8);\n",
    "  }\n",
    "\n",
    "  torch::Tensor forward(torch::Tensor x) {\n",
    "    // Group 1\n",
    "    x = torch::nn::functional::relu(             // depthwise separable 1\n",
    "        pconv1->forward(dconv1->forward(x))\n",
    "    );     \n",
    "    x = torch::nn::functional::relu(             // depthwise separable 2\n",
    "        pconv2->forward(dconv2->forward(x))\n",
    "    );    \n",
    "    \n",
    "    // Group 2\n",
    "    x = torch::max_pool2d(x, 2);                 // Sampling -> shape/2\n",
    "    x = torch::nn::functional::relu(             // depthwise separable 3\n",
    "        pconv3->forward(dconv3->forward(x))\n",
    "    );     \n",
    "    x = torch::nn::functional::relu(             // depthwise separable 4\n",
    "        pconv4->forward(dconv4->forward(x))\n",
    "    );     \n",
    "    \n",
    "    // Group 3\n",
    "    x = torch::max_pool2d(x, 2);                 // Sampling -> shape/4\n",
    "    x = pconv5->forward(dconv5->forward(x));     // depthwise separable 5\n",
    "    x = torch::nn::functional::relu(\n",
    "        torch::nn::functional::instance_norm(x)\n",
    "    ); \n",
    "    \n",
    "    // Group 4\n",
    "    x = torch::nn::functional::interpolate(      // UpSampling -> shape/2\n",
    "        x, \n",
    "        torch::nn::functional::InterpolateFuncOptions()\n",
    "            .scale_factor(std::vector<double>({2.0, 2.0}))\n",
    "            .mode(torch::kNearest)\n",
    "    );\n",
    "    x = pconv6->forward(dconv6->forward(x));     // depthwise separable 6\n",
    "    x = torch::nn::functional::relu(             // Instance norm 1 + ReLU\n",
    "        torch::nn::functional::instance_norm(x)\n",
    "    ); \n",
    "\n",
    "    // Group 5\n",
    "    x = torch::nn::functional::interpolate(      // UpSampling -> shape\n",
    "        x, \n",
    "        torch::nn::functional::InterpolateFuncOptions()\n",
    "            .scale_factor(std::vector<double>({2.0, 2.0}))\n",
    "            .mode(torch::kNearest)\n",
    "    );\n",
    "    x = pconv7->forward(dconv7->forward(x));     // depthwise separable 7\n",
    "    x = torch::nn::functional::relu(             // Instance norm 2 + ReLU\n",
    "        torch::nn::functional::instance_norm(x)\n",
    "    ); \n",
    "    \n",
    "    // Group 6\n",
    "    x = torch::sigmoid(\n",
    "        pconv8->forward(dconv8->forward(x))\n",
    "    );\n",
    "      \n",
    "    return x;\n",
    "  }\n",
    "    \n",
    "  // Pre-defined layers\n",
    "  torch::nn::Conv2d dconv1;  // The depthwise separable layer #1\n",
    "  torch::nn::Conv2d pconv1;\n",
    "  torch::nn::Conv2d dconv2;  // The depthwise separable layer #2\n",
    "  torch::nn::Conv2d pconv2;\n",
    "  torch::nn::Conv2d dconv3;  // The depthwise separable layer #3\n",
    "  torch::nn::Conv2d pconv3;\n",
    "  torch::nn::Conv2d dconv4;  // The depthwise separable layer #4\n",
    "  torch::nn::Conv2d pconv4;\n",
    "  torch::nn::Conv2d dconv5;  // The depthwise separable layer #5\n",
    "  torch::nn::Conv2d pconv5;\n",
    "  torch::nn::Conv2d dconv6;  // The depthwise separable layer #6\n",
    "  torch::nn::Conv2d pconv6;\n",
    "  torch::nn::Conv2d dconv7;  // The depthwise separable layer #7\n",
    "  torch::nn::Conv2d pconv7;\n",
    "  torch::nn::Conv2d dconv8;  // The depthwise separable layer #8\n",
    "  torch::nn::Conv2d pconv8;\n",
    "};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13954045",
   "metadata": {},
   "source": [
    "## Loss and accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84e5ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Loss function\n",
    "torch::Tensor model_loss(torch::Tensor& y_pred, torch::Tensor& y_true){\n",
    "    torch::nn::BCELoss loss_criterion;\n",
    "    return loss_criterion(y_pred, y_true);\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cc8f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Accuracy\n",
    "torch::Tensor model_accuracy(torch::Tensor& y_pred, torch::Tensor& y_true){\n",
    "    torch::Tensor round_pred = torch::round(y_pred);\n",
    "    return torch::mean(torch::eq(round_pred, y_true).to(torch::kFloat32));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6cee99",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a7ac2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "std::map <std::string, std::vector<std::string> > cdnet_data;\n",
    "\n",
    "//intermittentObjectMotion --> streetLight\n",
    "\n",
    "cdnet_data[\"badWeather\"]               = std::vector<std::string>{\"blizzard\",\"skating\",\"snowFall\",\"wetSnow\"};\n",
    "cdnet_data[\"baseline\"]                 = std::vector<std::string>{\"highway\",\"office\",\"pedestrians\",\"PETS2006\"};\n",
    "cdnet_data[\"cameraJitter\"]             = std::vector<std::string>{\"badminton\",\"boulevard\",\"sidewalk\",\"traffic\"};\n",
    "cdnet_data[\"dynamicBackground\"]        = std::vector<std::string>{\"boats\",\"canoe\",\"fall\",\"fountain01\",\"fountain02\",\"overpass\"};\n",
    "cdnet_data[\"intermittentObjectMotion\"] = std::vector<std::string>{\"abandonedBox\",\"parking\",\"sofa\",\"tramstop\",\"winterDriveway\"};\n",
    "cdnet_data[\"lowFramerate\"]             = std::vector<std::string>{\"port_0_17fps\",\"tramCrossroad_1fps\",\"tunnelExit_0_35fps\",\"turnpike_0_5fps\"};\n",
    "cdnet_data[\"nightVideos\"]              = std::vector<std::string>{\"bridgeEntry\",\"busyBoulvard\",\"fluidHighway\",\"streetCornerAtNight\",\"tramStation\",\"winterStreet\"};\n",
    "cdnet_data[\"PTZ\"]                      = std::vector<std::string>{\"continuousPan\",\"intermittentPan\",\"twoPositionPTZCam\",\"zoomInZoomOut\"};\n",
    "cdnet_data[\"shadow\"]                   = std::vector<std::string>{\"backdoor\",\"bungalows\",\"busStation\",\"copyMachine\",\"cubicle\",\"peopleInShade\"};\n",
    "cdnet_data[\"thermal\"]                  = std::vector<std::string>{\"corridor\",\"diningRoom\",\"lakeSide\",\"library\",\"park\"};\n",
    "cdnet_data[\"turbulence\"]               = std::vector<std::string>{\"turbulence0\",\"turbulence1\",\"turbulence2\",\"turbulence3\"};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd89dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Config {\n",
    "    bool            use_multi_gpus                  = true;\n",
    "    std::string     sequence_dir                    = \"data/cdnet\";\n",
    "    std::string     foreground_training_data_dir    = \"data/fg200\";\n",
    "    std::string     scenario_name                   = \"badWeather\";\n",
    "    std::string     sequence_name                   = \"skating\";\n",
    "    \n",
    "    int             kFPS\t\t\t\t\t\t\t= 240;\n",
    "    int             kMixture\t\t\t\t\t\t= 4;\n",
    "    int             kPixelBatch\t\t\t\t\t\t= 128;\n",
    "    \n",
    "    float           kLearningRate\t\t\t\t\t= 1e-4;\n",
    "    int             kEpochs\t\t\t\t\t\t\t= 1;\n",
    "    int             kSlidingStep\t\t\t\t\t= 2;\n",
    "    \n",
    "    std::string\t\tCKPT_dir\t\t\t\t\t\t= \"training_ckpt\";\n",
    "    std::string\t\tFG_TRAINING_DATA\t\t\t\t= \"data/fg_train_data\";    \n",
    "    std::string\t\tFG_TRAINING_FRAME\t\t\t\t= \"data/fg_train_frame\";    \n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca42b65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proc_id #0: dataset_size = 200\n"
     ]
    }
   ],
   "source": [
    "// Create config for the model\n",
    "Config config;\n",
    "\n",
    "// Initialize custom dataset for foreground training\n",
    "int proc_id    = 0;\n",
    "int n_proc     = 1;\n",
    "int n_replicas = 1;\n",
    "auto dataset = CustomDataset(\n",
    "    proc_id,                     // proc_id\n",
    "    n_proc,                      // num_proc   \n",
    "    n_replicas,                  // n_replicas\n",
    "    config.FG_TRAINING_FRAME,    // data_dir\n",
    "    config.scenario_name,        // scenario_name\n",
    "    config.sequence_name         // sequence_name\n",
    ").map(torch::data::transforms::Stack<>());\n",
    "\n",
    "// Generate a data loader.\n",
    "int batch_size = 8;\n",
    "auto data_loader = torch::data::make_data_loader<torch::data::samplers::RandomSampler>(\n",
    "    dataset,\n",
    "    batch_size\n",
    ");\n",
    "\n",
    "// Toogle dataset size:\n",
    "int dataset_size = dataset.size().value();\n",
    "std::cout << \"Proc_id #\" << proc_id << \": dataset_size = \" << dataset_size << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d66f1a",
   "metadata": {},
   "source": [
    "### Set device: CPU or GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcdeee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set device CPU or GPU (if available)\n",
    "torch::Device device(torch::cuda::is_available() ? \n",
    "                         /*torch::DeviceType=*/ torch::kCUDA :\n",
    "                         /*torch::DeviceType=*/ torch::kCPU, \n",
    "                     /*int device_idx=*/ 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68b2a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Init model and put model to device\n",
    "Model fdn_net;\n",
    "fdn_net.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0fa9d6ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sz = 16\n",
      "_param.key() = dconv1.weight\n",
      "\t\t_param.value().sizes() = [6, 1, 3, 3]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 54\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = pconv1.weight\n",
      "\t\t_param.value().sizes() = [16, 6, 1, 1]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 96\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = dconv2.weight\n",
      "\t\t_param.value().sizes() = [16, 1, 3, 3]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 144\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = pconv2.weight\n",
      "\t\t_param.value().sizes() = [16, 16, 1, 1]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 256\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = dconv3.weight\n",
      "\t\t_param.value().sizes() = [16, 1, 3, 3]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 144\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = pconv3.weight\n",
      "\t\t_param.value().sizes() = [16, 16, 1, 1]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 256\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = dconv4.weight\n",
      "\t\t_param.value().sizes() = [16, 1, 3, 3]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 144\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = pconv4.weight\n",
      "\t\t_param.value().sizes() = [16, 16, 1, 1]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 256\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = dconv5.weight\n",
      "\t\t_param.value().sizes() = [16, 1, 3, 3]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 144\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = pconv5.weight\n",
      "\t\t_param.value().sizes() = [16, 16, 1, 1]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 256\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = dconv6.weight\n",
      "\t\t_param.value().sizes() = [16, 1, 3, 3]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 144\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = pconv6.weight\n",
      "\t\t_param.value().sizes() = [16, 16, 1, 1]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 256\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = dconv7.weight\n",
      "\t\t_param.value().sizes() = [16, 1, 3, 3]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 144\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = pconv7.weight\n",
      "\t\t_param.value().sizes() = [16, 16, 1, 1]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 256\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = dconv8.weight\n",
      "\t\t_param.value().sizes() = [16, 1, 3, 3]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 144\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n",
      "_param.key() = pconv8.weight\n",
      "\t\t_param.value().sizes() = [1, 16, 1, 1]\n",
      "\t\t_param.value().dim() = 4\n",
      "\t\t_param.value().numel() = 16\n",
      "\t\t_param.value().element_size() = 4\n",
      "\t\t_param.dtype() = float\n"
     ]
    }
   ],
   "source": [
    "// Read more about torch::OrderedDict< Key, Value > at:\n",
    "// https://caffe2.ai/doxygen-c/html/classtorch_1_1_ordered_dict.html\n",
    "auto param = fdn_net.named_parameters(); // torch::OrderedDict< Key, Value >\n",
    "int sz = param.size();\n",
    "\n",
    "std::cout << \"sz = \" << sz << std::endl;\n",
    "\n",
    "for(auto _param : param){\n",
    "    const std::string& _param_name = _param.key();\n",
    "    torch::Tensor& _param_value = _param.value();\n",
    "    \n",
    "    std::cout << \"_param.key() = \" << _param_name << \"\\n\" // can be said as Param's name\n",
    "              << \"\\t\\t\" << \"_param.value().sizes() = \" << _param_value.sizes() << \"\\n\" // shape of param's tensor\n",
    "              << \"\\t\\t\" << \"_param.value().dim() = \" << _param_value.dim() << \"\\n\" // size of weight\n",
    "              << \"\\t\\t\" << \"_param.value().numel() = \" << _param_value.numel() << \"\\n\" // size of weight\n",
    "              << \"\\t\\t\" << \"_param.value().element_size() = \" << _param_value.element_size() << \"\\n\" // size of dtype\n",
    "              << \"\\t\\t\" << \"_param.dtype() = \" << _param_value.dtype() << \"\\n\"; // size of dtype\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f22b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Init optimizer\n",
    "torch::optim::Adam optimizer(\n",
    "    fdn_net.parameters(), \n",
    "    torch::optim::AdamOptions(5e-1)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3db29dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 5\tLoss = 1.91458\tAcc = 0.614417\n",
      "step = 10\tLoss = 1.56815\tAcc = 0.655548\n",
      "step = 15\tLoss = 1.93174\tAcc = 0.728098\n",
      "step = 20\tLoss = 2.10939\tAcc = 0.763509\n",
      "step = 25\tLoss = 2.3056\tAcc = 0.777838\n"
     ]
    }
   ],
   "source": [
    "float loss_val = 0.0;\n",
    "float acc_val = 0.0;\n",
    "\n",
    "int n_sample_passed = 0;\n",
    "int n_iter = 1;\n",
    "int step = 0;\n",
    "int log_freq = 5;\n",
    "\n",
    "for(int iter=0;iter<n_iter;iter++){\n",
    "    // Iterate over data_loader\n",
    "    for (auto& batch : *data_loader) {\n",
    "        auto data = batch.data.to(torch::kFloat32).to(device) / 255.0;\n",
    "        auto target = batch.target.to(torch::kFloat32).to(device) / 255.0;\n",
    "\n",
    "        optimizer.zero_grad();\n",
    "\n",
    "        // Feed forward\n",
    "        torch::Tensor pred_output = fdn_net.forward(data);\n",
    "\n",
    "        // Loss estimation\n",
    "        torch::Tensor loss = model_loss(pred_output, target);\n",
    "\n",
    "        // Accuracy estimation\n",
    "        torch::Tensor acc = model_accuracy(pred_output, target);\n",
    "\n",
    "        // Backpropagation + weight update\n",
    "        loss.backward();\n",
    "        optimizer.step();\n",
    "\n",
    "        // Calculate loss & acc\n",
    "        n_sample_passed += data.sizes()[0];\n",
    "        loss_val += loss.item<float>();\n",
    "        acc_val += acc.item<float>();\n",
    "        \n",
    "        step++;\n",
    "        if((step+1)%log_freq == 0)\n",
    "            std::cout << \"step = \" << (step+1) << \"\\t\"\n",
    "                      << \"Loss = \" << loss_val/step << \"\\t\"\n",
    "                      << \"Acc = \" << acc_val/step << std::endl;\n",
    "    }\n",
    "    // cv::destroyAllWindows();\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "296c6b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 26\tLoss = 2.49763\tAcc = 0.836149\n"
     ]
    }
   ],
   "source": [
    "float loss_val = 0.0;\n",
    "float acc_val = 0.0;\n",
    "int step = 0;\n",
    "\n",
    "for(auto& batch : *data_loader){\n",
    "    auto data = batch.data.to(torch::kFloat32).to(device) / 255.0;\n",
    "    auto target = batch.target.to(torch::kFloat32).to(device) / 255.0;\n",
    "\n",
    "    // Feed forward\n",
    "    torch::Tensor pred_output = fdn_net.forward(data);\n",
    "        \n",
    "    // Loss estimation\n",
    "    torch::Tensor loss = model_loss(pred_output, target);\n",
    "\n",
    "    // Accuracy estimation\n",
    "    torch::Tensor acc = model_accuracy(pred_output, target);\n",
    "    \n",
    "    // Calculate loss & acc\n",
    "    step++;\n",
    "    loss_val += loss.item<float>();\n",
    "    acc_val += acc.item<float>();    \n",
    "}\n",
    "\n",
    "std::cout << \"step = \" << (step+1) << \"\\t\"\n",
    "          << \"Loss = \" << loss_val/step << \"\\t\"\n",
    "          << \"Acc = \" << acc_val/step << std::endl;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97dc2661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO:0] global /home/hung/Downloads/opencv-4.5.2/modules/core/src/parallel/registry_parallel.impl.hpp (96) ParallelBackendRegistry core(parallel): Enabled backends(2, sorted by priority): TBB(1000); OPENMP(990)\n",
      "[ INFO:0] global /home/hung/Downloads/opencv-4.5.2/modules/core/include/opencv2/core/parallel/backend/parallel_for.tbb.hpp (54) ParallelForBackend Initializing TBB parallel backend: TBB_INTERFACE_VERSION=9107\n",
      "[ INFO:0] global /home/hung/Downloads/opencv-4.5.2/modules/core/src/parallel/parallel.cpp (77) createParallelForAPI core(parallel): using backend: TBB (priority=1000)\n"
     ]
    }
   ],
   "source": [
    "int test_idx = 0;\n",
    "for(auto& batch : *data_loader){\n",
    "    auto data = batch.data.to(torch::kFloat32).to(device) / 255.0;\n",
    "    auto target = batch.target;\n",
    "\n",
    "    // Feed forward\n",
    "    torch::Tensor pred_output = torch::round(fdn_net.forward(data));\n",
    "    torch::Tensor output = (pred_output*255)\n",
    "                            .to(torch::kCPU).to(torch::kUInt8)\n",
    "                            .permute({0,2,3,1}).squeeze().contiguous();\n",
    "        \n",
    "    auto target_ = target.permute({0,2,3,1}).squeeze().contiguous();\n",
    "    \n",
    "    for(int j=0;j<data.sizes()[0];j++){\n",
    "        torch::Tensor sample = output.index({j, \"...\"});\n",
    "        torch::Tensor gt = target_.index({j, \"...\"});\n",
    "        cv::Mat pred_mat = cvt_torchTensor_to_cvMat(sample);\n",
    "        cv::Mat gt_mat = cvt_torchTensor_to_cvMat(gt);\n",
    "        \n",
    "        cv::imshow(\"pred_mat\", pred_mat);\n",
    "        cv::imshow(\"gt_mat\", gt_mat);\n",
    "        \n",
    "        cv::waitKey(800);\n",
    "        \n",
    "    }\n",
    "    \n",
    "    if (++test_idx == 7)\n",
    "        break;\n",
    "}\n",
    "cv::destroyAllWindows();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a08d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
